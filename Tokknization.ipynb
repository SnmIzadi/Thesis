{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f7143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5819aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('nl_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880fdb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low confidence in encoding detection for 28BC.txt. Defaulting to utf-8.\n",
      "Tokenization and NER complete for all files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "input_directory = '/Users/sanam/Desktop/thesis/ONLYPO'\n",
    "output_directory = '/Users/sanam/Desktop/thesis/tokcpyPO'\n",
    "ner_output_directory = '/Users/sanam/Desktop/thesis/khodesh/nerPO'\n",
    "stopwords_path = \"/Users/sanam/Desktop/thesis/stopwords-nl.txt\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "os.makedirs(ner_output_directory, exist_ok=True)\n",
    "brin_csv_path = '/Users/sanam/Desktop/thesis/school_data.csv'\n",
    "quality_df = pd.read_csv(brin_csv_path) \n",
    "quality_df = quality_df[['Brin', 'KwaliteiOnderwijs']]\n",
    "quality_df = quality_df[quality_df['KwaliteiOnderwijs'] != 'Geen eindoordeel']\n",
    "\n",
    "# Load stopwords from file\n",
    "with open(stopwords_path, 'r', encoding='utf-8') as file:\n",
    "    stopwords = set(file.read().split())\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        text = None\n",
    "        \n",
    "        # Try reading the file with detected encoding\n",
    "        try:\n",
    "            brin_value = filename.replace(\".txt\", \"\")\n",
    "            quality_rating = quality_df.loc[quality_df['Brin'] == brin_value, 'KwaliteiOnderwijs']\n",
    "            if quality_rating.empty or quality_rating.values[0] == 'Geen eindoordeel':\n",
    "                continue  # Skip this file\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read()\n",
    "                result = chardet.detect(rawdata)\n",
    "                encoding = result['encoding']\n",
    "                confidence = result['confidence']\n",
    "                \n",
    "                # If confidence is too low, default to utf-8\n",
    "                if encoding is None or confidence < 0.7:\n",
    "                    print(f\"Low confidence in encoding detection for {filename}. Defaulting to utf-8.\")\n",
    "                    encoding = 'utf-8'\n",
    "            \n",
    "            with open(file_path, 'r', encoding=encoding, errors='replace') as file:\n",
    "                text = file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Skipping {filename} due to encoding issues.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if text:\n",
    "            # Tokenize the text\n",
    "            doc = nlp(text)\n",
    "            tokens = []\n",
    "            for token in doc:\n",
    "                # Check if the token is a stopword, punctuation, space, digit, or a number-like token\n",
    "                if (token.is_stop or token.is_punct or token.is_space or \n",
    "                    token.is_digit or token.like_num or token.text.lower() in stopwords or\n",
    "                    re.match(r'^\\d', token.text)):\n",
    "                    continue\n",
    "                \n",
    "                # Exclude tokens that are part of entities related to dates or numbers\n",
    "                if token.ent_type_ in [\"DATE\", \"CARDINAL\", \"ORDINAL\", \"QUANTITY\", \"PERCENT\", \"MONEY\", \"TIME\"]:\n",
    "                    continue\n",
    "\n",
    "                # Exclude tokens that contain any digit\n",
    "                if re.search(r'\\d', token.text):\n",
    "                    continue\n",
    "                \n",
    "                # Exclude tokens that are not alphabetic or are too short\n",
    "                if not token.text.isalpha() or len(token.text) < 4:\n",
    "                    continue\n",
    "                if token in stopwords:\n",
    "                    continue\n",
    "                \n",
    "                # Lemmatize the token\n",
    "                lemma = token.lemma_\n",
    "                \n",
    "                # Exclude lemmas that are stopwords\n",
    "                \n",
    "                tokens.append(lemma)\n",
    "            \n",
    "            # Save tokenized text\n",
    "            if tokens:\n",
    "                output_file_path = os.path.join(output_directory, f\"{filename}\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write('\\n'.join(tokens))\n",
    "            else:\n",
    "                print(f\"No valid tokens were found in {filename}.\")\n",
    "\n",
    "print(\"Tokenization and NER complete for all files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73db84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eca48ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
